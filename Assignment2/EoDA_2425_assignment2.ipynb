{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pVCR4GznXPBd"
      },
      "source": [
        "# Engineering of Data Analysis: assignment 2\n",
        "\n",
        "By delivering this notebook, we confirm that the code presented was developed by the following students.\n",
        "\n",
        "**Student num:** TBC     **; Name:**\n",
        "\n",
        "**Student num:** TBC     **; Name:** TBC\n",
        "\n",
        "**DEADLINE:** 13th May, 23h59\n",
        "\n",
        "**Only one student should deliver the notebook in a ZIP file that includes the dataset for exercise 4**\n",
        "**The notebook shoud have all outputs present**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xyM6HefJXPBe"
      },
      "source": [
        "Some useful links:\n",
        "* [ACM DEBS 2015 Grand Challenge](http://www.debs2015.org/call-grand-challenge.html)\n",
        "\n",
        "* [Spark web site](https://spark.apache.org/)\n",
        "\n",
        "* [Anonymity_api](https://github.com/farosilva0/anonymity_api)\n",
        "\n",
        "* [IBM differential privacy library](https://github.com/IBM/differential-privacy-library)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MhyO9-O2wuoa"
      },
      "source": [
        "# Install software"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uDH5ObnQwtRy"
      },
      "outputs": [],
      "source": [
        "# Install Spark\n",
        "!apt-get install openjdk-17-jdk-headless\n",
        "!pip install pyspark==4.0.0.dev2\n",
        "!mkdir checkpoint\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install anonymity-api==1.0.4\n",
        "!pip install diffprivlib\n"
      ],
      "metadata": {
        "id": "TEdQIWsufh18"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bc9cRxyLxzDv"
      },
      "source": [
        "# Setup\n",
        "\n",
        "The data sets are available in the following link: https://drive.google.com/drive/folders/1WMwLUj0t4Q0GSll96lbF2bDjaPVh1w8z?usp=sharing (the same as in assignment 1). For running in Google Colab, you should access the link and Add Shortcut to your Drive.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zoe8_ojZxyOL"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# If you added the shortcut to your drive, the file should appear in this directory\n",
        "# If not, you need to explore from directory /content/drive\n",
        "!ls /content/drive/MyDrive/assignment1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b-NC3KtagnWM"
      },
      "outputs": [],
      "source": [
        "# Run this cell only if you are going to run exercise 1\n",
        "!mkdir data\n",
        "!cp /content/drive/MyDrive/assignment1/sample.csv data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6G8r7sp9XPBe"
      },
      "source": [
        "Next cells setup: Pandas and Spark."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WPGlacioYpf-"
      },
      "outputs": [],
      "source": [
        "# General imports\n",
        "import timeit\n",
        "\n",
        "import os\n",
        "import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import sklearn.cluster\n",
        "\n",
        "from anonymity_api import anonymity\n",
        "from anonymity_api import utility"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9tmUtWFKZt6e"
      },
      "outputs": [],
      "source": [
        "# Imports for Spark\n",
        "\n",
        "import os\n",
        "os.environ[\"PYARROW_IGNORE_TIMEZONE\"] = \"1\"\n",
        "\n",
        "# imports Spark SQL and Pandas API\n",
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.ml.linalg import Vectors\n",
        "from pyspark.ml.clustering import KMeans\n",
        "from pyspark.ml.evaluation import *\n",
        "from pyspark.ml.feature import *\n",
        "import pyspark.pandas as ps\n",
        "\n",
        "# this sets the maximum number of rows to show when printing pandas Dataframes\n",
        "ps.set_option('display.max_rows', 10)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b-KW2JrJbXai"
      },
      "outputs": [],
      "source": [
        "FILENAME_LOCAL = \"data/sample.csv\"\n",
        "FILENAME_RMT = \"/content/drive/MyDrive/assignment1/sample.csv\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RAm-BRKXXPBg"
      },
      "source": [
        "## Exercise 1: local data files vs. remote data files\n",
        "\n",
        "Compute the number of distinct licenses, accessing local file and remote file.\n",
        "\n",
        "Compare the time it takes to execute the code for Pandas, Spark SQL and cuDF for accessing a local file and a remote file.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I--Vmbx8XPBg"
      },
      "source": [
        "### Code: Pandas library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "khxF5l8VXPBg"
      },
      "outputs": [],
      "source": [
        "mySchema = [\"medallion\", \"hack_license\", \"pickup_datetime\",\n",
        "            \"dropoff_datetime\", \"trip_time_in_secs\", \"trip_distance\",\n",
        "            \"pickup_longitude\", \"pickup_latitude\", \"dropoff_longitude\",\n",
        "            \"dropoff_latitude\", \"payment_type\", \"fare_amount\",\n",
        "            \"surcharge\", \"mta_tax\", \"tip_amount\",\n",
        "            \"tolls_amount\", \"total_amount\"]\n",
        "\n",
        "print( \"Local file\")\n",
        "start_time = time.time()\n",
        "\n",
        "dataset = pd.read_csv(FILENAME_LOCAL,names=mySchema)\n",
        "result = dataset[\"hack_license\"].nunique()\n",
        "\n",
        "end_time = time.time()\n",
        "\n",
        "print( \"Runtime = \" + str(end_time - start_time))\n",
        "\n",
        "\n",
        "print( \"Remote file\")\n",
        "start_time = time.time()\n",
        "\n",
        "dataset = pd.read_csv(FILENAME_RMT,names=mySchema)\n",
        "result = dataset[\"hack_license\"].nunique()\n",
        "\n",
        "end_time = time.time()\n",
        "\n",
        "print( \"Runtime = \" + str(end_time - start_time))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6cgoZCdXPBg"
      },
      "source": [
        "### Results (Pandas)\n",
        "\n",
        "The time to process the small dataset, local file, was : **TO COMPLETE WITH 3 MEASUREMENTS** seconds.\n",
        "\n",
        "The time to process the small dataset, remote file, was : **TO COMPLETE WITH 3 MEASUREMENTS** seconds.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPo82rOWXPBg"
      },
      "source": [
        "### Spark SQL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l2On5zrlXPBg"
      },
      "outputs": [],
      "source": [
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .appName(\"Group project\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "mySchema = StructType([\n",
        "    StructField(\"medallion\", StringType()),\n",
        "    StructField(\"hack_license\", StringType()),\n",
        "    StructField(\"pickup_datetime\", TimestampType()),\n",
        "    StructField(\"dropoff_datetime\", TimestampType()),\n",
        "    StructField(\"trip_time_in_secs\", IntegerType()),\n",
        "    StructField(\"trip_distance\", DoubleType()),\n",
        "    StructField(\"pickup_longitude\", DoubleType()),\n",
        "    StructField(\"pickup_latitude\", DoubleType()),\n",
        "    StructField(\"dropoff_longitude\", DoubleType()),\n",
        "    StructField(\"dropoff_latitude\", DoubleType()),\n",
        "    StructField(\"payment_type\", StringType()),\n",
        "    StructField(\"fare_amount\", DoubleType()),\n",
        "    StructField(\"surcharge\", DoubleType()),\n",
        "    StructField(\"mta_tax\", DoubleType()),\n",
        "    StructField(\"tip_amount\", DoubleType()),\n",
        "    StructField(\"tolls_amount\", DoubleType()),\n",
        "    StructField(\"total_amount\", DoubleType()),\n",
        "])\n",
        "\n",
        "print( \"Local file\")\n",
        "\n",
        "start_time = time.time()\n",
        "dataset = spark.read.load(FILENAME_LOCAL, format=\"csv\",\n",
        "                         sep=\",\", schema=mySchema, header=\"false\")\n",
        "dataset.createOrReplaceTempView(\"data\")\n",
        "statisticsDF = spark.sql( \"\"\"SELECT COUNT(DISTINCT hack_license) AS total_amount FROM data\"\"\")\n",
        "statistics = statisticsDF.collect()\n",
        "\n",
        "end_time = time.time()\n",
        "\n",
        "print( \"Runtime = \" + str(end_time - start_time))\n",
        "\n",
        "print( \"Remote file\")\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "dataset = spark.read.load(FILENAME_RMT, format=\"csv\",\n",
        "                         sep=\",\", schema=mySchema, header=\"false\")\n",
        "dataset.createOrReplaceTempView(\"data\")\n",
        "statisticsDF = spark.sql( \"\"\"SELECT COUNT(DISTINCT hack_license) AS total_amount FROM data\"\"\")\n",
        "statistics = statisticsDF.collect()\n",
        "\n",
        "end_time = time.time()\n",
        "\n",
        "print( \"Runtime = \" + str(end_time - start_time))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jkrh7oezXPBg"
      },
      "source": [
        "### Results (Spark)\n",
        "\n",
        "The time to process the small dataset, local file, was : **TO COMPLETE WITH 3 MEASUREMENTS** seconds.\n",
        "\n",
        "The time to process the small dataset, remote file, was : **TO COMPLETE WITH 3 MEASUREMENTS** seconds.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OAe85kD0XPBg"
      },
      "source": [
        "### Results discussion\n",
        "\n",
        "**TO BE COMPLETED after lecture 8/May**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 2\n",
        "\n",
        "The goal of this exercise is to compare results obtained with anonymized data and not anonymized. We will use a dataset with information about hear disease.\n",
        "\n",
        "The quasi-identifiers are composed by attributes:\n",
        "\n",
        "    * age - Age of the patient\n",
        "    * sex - Gender of the patient\n",
        "    * cp - Type of chest pain\n",
        "    * trestbps - resting blood pressure\n",
        "    * chol - cholestoral\n",
        "    * fbs - fasting blood sugar > 120 mg/dl\n",
        "    * restecg - resting eletrocardiographic results\n",
        "    * thalach - Maximum heart rate\n",
        "    * exang - Exercise induced angina\n",
        "    * oldpeak - ST depression induced by exercise relative to rest\n",
        "    * slope - Slope of peak exercise ST segment\n",
        "    * ca - Number of major vessels colored by fluoroscopy\n",
        "    * thal - Thalassemia\n",
        "\n",
        "The sensitive-attributes are composed by only one attribute:\n",
        "\n",
        "    * target - 1 if the patient has Hear Disease, otherwise 0"
      ],
      "metadata": {
        "id": "Q9hnnztRd99w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Load the dataset\n",
        "df = pd.read_csv('/content/drive/MyDrive/assignment1/heart.csv')\n",
        "\n",
        "# List with the quasi-identifiers\n",
        "qis = ['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal']\n",
        "\n",
        "# List with the sensitive attributes\n",
        "sas = ['target']\n"
      ],
      "metadata": {
        "id": "e4lcaYHOeB9I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's anonymize the dataset with library anonimity_lib."
      ],
      "metadata": {
        "id": "5zQ07nAs83p7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# the library can automatically suggest an anonymization without the need for providing parameters\n",
        "anon_df = anonymity.suggest_anonymity( df, qis, sas)\n",
        "\n",
        "# processing information after generalization is not very simple\n",
        "# the following function generate random value that keep the averahe inside each group\n",
        "# used for anonymity\n",
        "anon_gen_df = utility.generalize_intervals(anon_df, qis)"
      ],
      "metadata": {
        "id": "cplrXLGk9HrK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Workload-aware anonymization techniques tailor the anonymization perform to the workload that will be run. For knowing more, you can check:\n",
        "\n",
        "Kristen LeFevre, David J. DeWitt, and Raghu Ramakrishnan. 2006. Workload-aware anonymization. In Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining (KDD '06). Association for Computing Machinery, New York, NY, USA, 277–286.\n",
        "https://pages.cs.wisc.edu/~lefevre/WorkloadAware.pdf\n",
        "\n",
        "Anonimity_lib supports workload-aware anonymization: https://github.com/farosilva0/anonymity_api\n"
      ],
      "metadata": {
        "id": "2I131Zt9-CXK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# for queries based on intervals, you can do the following\n",
        "\n",
        "anon_groups_df = anonymity.suggest_anonymity_groups( df, qis, sas, queries=['group(age, 20)'])\n",
        "anon_groups_gen_df = utility.generalize_intervals(anon_groups_df, qis)"
      ],
      "metadata": {
        "id": "uVUyTWlKANpJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Propose and compte some statistics based on groups/bins, using the original dataset, the anonymized dataset, the workload-aware anonymized dataset and a  and compare the quality of results.\n"
      ],
      "metadata": {
        "id": "RZkC_dv5Ad8L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO"
      ],
      "metadata": {
        "id": "rt0QnWB6AXHx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Results discussion\n",
        "\n",
        "**TO BE COMPLETED - start by explaining the statistics you are computing**\n"
      ],
      "metadata": {
        "id": "MeU9FPxiCiLm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 3\n",
        "\n",
        "The goal of this exercise is the same as before, but with a workload where the correlation between values is important.\n",
        "\n",
        "We will be using the Life Expectancy Dataset. (https://www.kaggle.com/datasets/kumarajarshi/life-expectancy-who)\n",
        "\n",
        "From this dataset we will be using the following attributes:\n",
        "\n",
        "- Year: Year relevant to the tuple\n",
        "- Status: If the country in the tuple is Developed or Developing\n",
        "- Life expectancy: The life expectancy in age\n",
        "- Adult Mortality: Moratlity rates (Probability of dying between the age of 15 and 60 per 1000 population)\n",
        "- Infant deaths: Number of infant deaths per 1000 population\n",
        "- Alcohol: Alcohol comsuption per capita (in litres)\n",
        "- Hepatitis B: Hepatitis B immunization coverage aming 1-year olds (percentage)\n",
        "- Measles: Number of reported meales cases per 1000 population\n",
        "- BMI: Average Body Mass Index of the population\n",
        "- Total Expenditure: General Government expenditure on health, as a percentage of total government expenditure\n",
        "\n",
        "From these attributes, we will be using the Life expectancy an the sensitive attribute, with the remaining attributes as quasi-identifiers"
      ],
      "metadata": {
        "id": "CHlIqS94EwOU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "data = pd.read_csv('/content/drive/MyDrive/assignment1/Life_Expectancy_Data.csv')\n",
        "data['Status'] = data['Status'].replace({'Developing': 0, 'Developed': 1})\n",
        "sas = ['Life expectancy ']\n",
        "qis = ['Year', 'Status', 'Alcohol', 'Hepatitis B', 'Measles ', ' BMI ', 'Total expenditure', 'Adult Mortality', 'infant deaths']\n",
        "data = data.dropna()\n"
      ],
      "metadata": {
        "id": "8WjzlkSjwd0Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For anonymizing, keeping correlations, you can use the following code."
      ],
      "metadata": {
        "id": "h8sfwPV5F8Zk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corr_anon = anonymity.workload_aware_k_anonymity(data, qis, 5, queries=['corr(Adult Mortality, Life expectancy )'])\n",
        "gen_corr = utility.generalize_intervals(corr_anon, qis, decimals=['Alcohol', 'BMI', 'Total expenditure'])\n"
      ],
      "metadata": {
        "id": "EHlaUxpAw4Tn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compare the original dataset, the workload-aware anonymized dataset and the dataset anonymized with differential privacy for computing some form of regression."
      ],
      "metadata": {
        "id": "u9JWi4QrGHhT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO complete with code for original dataset"
      ],
      "metadata": {
        "id": "ujmro39TGHCk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO complete with code with workload-aware anonization"
      ],
      "metadata": {
        "id": "Ael2WOP4INE7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO complete with code with differential privacy"
      ],
      "metadata": {
        "id": "b-Z1JrYJIT5k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Results discussion\n",
        "\n",
        "**TO BE COMPLETED**\n"
      ],
      "metadata": {
        "id": "3SvKV6MDG-bk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 4\n",
        "\n",
        "The goal of this exercise is to compare results obtained with anonymized and non anonymized data for more complex ML techniques.\n",
        "\n",
        "Repeat some code you have done in ML (or some other) course using the original dataset and an anonymization using differential privacy.\n"
      ],
      "metadata": {
        "id": "ubhoceMMwblp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ML task performed\n",
        "\n",
        "Briefly explain the task you are implementing."
      ],
      "metadata": {
        "id": "kOmF1jPSHg1Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO - code without differential privacy"
      ],
      "metadata": {
        "id": "HHFRG81sHtyR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO - code with differential privacy"
      ],
      "metadata": {
        "id": "-dOE04ftH1hf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO - comaprison of results"
      ],
      "metadata": {
        "id": "tBPXBafJH3TJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Results discussion\n",
        "\n",
        "**TO BE COMPLETED**"
      ],
      "metadata": {
        "id": "i9w2KfqIH5zt"
      }
    }
  ],
  "metadata": {
    "application/vnd.databricks.v1+notebook": {
      "dashboards": [],
      "language": "python",
      "notebookName": "Group project resolution - template final",
      "notebookOrigID": 149761708232030,
      "widgets": {}
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}